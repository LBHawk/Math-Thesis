Here we overview some elementary topological concepts about metric spaces.  We specifically talk about metric and vector spaces, norms, and the completion of metric spaces.  Note that many definitions, theorems, etc. come from Gerald Edgar's \textit{Measure, Topology, and Fractal Geometry}\cite{Edgar} and Kaplansky's \textit{Set Theory and Metric Spaces}\cite{Kaplansky}.

\begin{defn}
A \textbf{metric space} is a set $S$ together with a function $d:S \times S \textrm{ if and only if } [0, \infty)$ satisfying the following:
\begin{align*}
\textrm{(1)}\; d(x,y) &= 0 \Leftrightarrow x = y\\
\textrm{(2)}\; d(x,y) &\geq 0 \textrm{ for all } x,y \in X\\
\textrm{(3)}\; d(x,y) &= d(y,x)\\
\textrm{(4)}\; d(x,z) &\leq d(x,y) + d(y,z) \textrm{ (Triangle inequality)}
\end{align*}
The nonnegative real number $d(x,y)$ is called the \textit{distance} between $x$ and $y$, while the function $d$ itself is known as the \textit{metric} of the set $S$.  A metric space is written as $(S, d)$, but oftentimes the metric is implied and the space is simply referred to as $S$.  Note that the last property, the triangle inequality, is a very important property as it is used very often in proofs.
\end{defn}

\begin{example}
The set of real numbers $\mathbb{R}$, with $d: \mathbb{R} \times \mathbb{R} \rightarrow \mathbb{R}$ defined by
\[d(x,y) = |x - y| \]
is a metric space.  This is the usual metric used with $\mathbb{R}$.
\end{example}

The complex plane $\mathbb{C}$ has a similar usual metric:

\begin{example}
The complex numbers $\mathbb{C}$ with $d: \mathbb{C} \times \mathbb{C} \rightarrow \mathbb{R}$ defined by
\[d(z,w) = |z - w| \]
where $|z|$ is the modulus of $z$ is a metric space.
\end{example}

Generally, algebraic operations are not defined on a metric space, just a distance function.  Meanwhile, a vector space (which is not necessarily a metric space) provides the operations of vector addition and scalar multiplication, but without a notion of distance.  We can combine a vector space with a \textit{norm}, though, to create a normed vector space --- note that all normed vector spaces are also metric spaces.

\begin{defn}
A \textbf{norm} on a vector space $V$ is a function $||x||: V \rightarrow [0, \infty) \subset \mathbb{R}$ which satisfies the following:
\begin{align*}
&\textrm{(1)}\; ||x|| \geq 0 \textrm{ and } ||x|| = 0 \textrm{ if and only if } x=0;\\
&\textrm{(2)}\; ||kx|| = |k|\,||x|| \textrm{ (scaling property)};\\
&\textrm{(3)}\; ||x + y|| \leq ||x|| + ||y||.
\end{align*}

A vector space $V$ together with a norm $||\cdot||$ is called a \textbf{normed vector space}, and is denoted $(V, ||\cdot||)$.
\end{defn}

The properties of a norm on a vector space are rather intuitive.  Property (1) says that a vector has nonnegative length, and the length of $x$ is 0 if and only if $x$ is the 0-vector; property (2) states multiplying a vector by a scalar $k$ multiplies its length by $k$; finally property (3) is the triangle inequality, which is analogous to property (4) of definition 3.1.

While a norm is defined rather similarly to a metric, the two are not the same.  However, we often define a metric on a normed metric space using the norm.

\begin{prop}
If $(X, ||\cdot||)$ is a normed vector space $X$, then $d: X \times X \rightarrow \mathbb{R}$, defined by $d(x,y) = ||x - y||$, is a metric on $X$.
\end{prop}

The necessary properties for $d$ to be a metric follow immediately from properties (1) and (3) of a norm.  If $X$ is a normed vector space, we always use the metric associated with its norm, unless specifically stated otherwise.

A metric defined on a norm has all the properties of a metric discussed earlier, as well as two more --- for all $x, y, z \in X$ and $k \in \mathbb{R}$
\[d(x+z, y+z) = d(x+y), \qquad d(kx, ky) = |k|d(x,y).\]

These properties are called \textit{translation invariance} and \textit{homogeneity}, respectively.  These properties are not included in a simple metric space because they do not even make sense in that framework --- recall that in a space which is only a metric space, we can not add points together or multiply them by scalars.

While there are a variety of norms which can be used on a vector space, the \textit{Euclidean norm} is the most common for $\mathbb{R}^n$ and is perhaps the most intuitive.

\begin{example}
On $\mathbb{R}^n$, the length of a vector $x = (x_1, x_2, ..., x_n)$ is given by
\[||x|| := \sqrt{x_1^2+x_2^2+\cdots+x_n^2}.\]
This gives the distance from the origin to the point $x$ and is known as the \textit{Euclidean norm}.  This should be familiar as it is the "straight-line" distance between points in space and is a result of the Pythagorean theorem.
\end{example}

\begin{example}
On $\mathbb{C}^n$, the most common norm is given by
\[||z|| := \sqrt{|z_1|^2 + |z_2|^2 + \cdots + |z_n|^2} = \sqrt{z_1\conj{z}_1 + z_2\conj{z}_2 + \cdots + z_n\conj{z}_n}\]
\end{example}

Note that while a metric is often derived from a norm, the existence of a metric does not imply a norm --- a metric does not even necessarily need to make geometric sense.  Take for example what is known as the \textit{discrete metric}:

\begin{example}
On any set $X$, the discrete metric is defined as
\[d(x,y)= 
\begin{cases} 
	0 & x=y, \\
	1 & x \neq y.\\
\end{cases}
\]
The discrete metric does not satisfy the homogeneity property we briefly discussed earlier, so we know this metric was not induced by a norm.
\end{example}

We now review a couple concepts about sequences in a metric space.  Many of the definitions and proofs in the rest of this section are based on excerpts from a real analysis textbook\cite{Abbott}, which we have adapted for the more general case of metric spaces.

\begin{defn}
A \textbf{sequence} is a function whose domain is a set of the form $\{n \in \mathbb{Z} : n \geq m\}$ for some $m \in \mathbb{Z}$.  A sequence is denoted $(s_n)_{n=m}^\infty$ or just $(s_n)$.
\end{defn}

\begin{example}
The following are a couple examples of sequences:
\begin{itemize}
\item Let $s_n = \frac{1}{n^2}, n\geq 1$.  Then, $(s_n) = (1, \frac{1}{4}, \frac{1}{9}, , \frac{1}{16}, \dots)$.
\item Let $s_n = \frac{1}{2}(1 + (-1)^n), n \geq 1$.  Then, $(s_n) = (0,1,0,1,\dots)$.
\end{itemize}
\end{example}

\begin{defn}
A sequence $(s_n)$  is \textit{nondecreasing} if $s_n \leq s_{n+1}$ for all $n$.  A sequence $(s_n)$  is \textit{nonincreasing} if $s_n \geq s_{n+1}$ for all $n$.  We call a sequence \textbf{monotonic} if it is either nonincreasing or nondecreasing.
\end{defn}

\begin{defn}
A sequence $(s_n)$ is \textbf{bounded} if there exists a number $M$ for which $||s_n|| < M$ for all $n$.
\end{defn}

\begin{example}
We provide a few examples of bounded and/or monotonic sequences:
\begin{itemize}
\item Let $s_n = \frac{1}{n^2}, n\geq 1$.  Then $(s_n)$ is both bounded and monotonic.
\item Let $s_n = \frac{1}{2}(1 + (-1)^n), n \geq 1$.  Then $(s_n)$ is bounded but not monotonic.
\item Let $s_n = n$.  Then $(s_n)$ is monotonic but unbounded.
\end{itemize}
\end{example}

\begin{theorem}
Every sequence has a monotonic subsequence.
\end{theorem}

The formal proof of this result is far more involved than what is necessary to understand why this is true.  As such, we only outline the general idea of how to construct such a subsequence in a very informal proof.

\begin{proof}[Informal proof]
Let $(s_n)_{n=1}^\infty$ be any sequence.  Define a term $s_n$ to be \textbf{dominant} if $s_n > s_m$ for all $m>n$.  Now, if $(s_n)$ has infinitely many dominant terms, we can construct a subsequence solely of dominant terms which is nonincreasing.  Similarly, if $(s_n)$ has a finite number of dominant terms, we can construct a subsequence solely out of terms beyone the last dominant term which is nondecreasing.  Thus, we can construct a monotonic subsequence for every sequence.
\end{proof}

\begin{defn}
A sequence $(s_n)$ \textbf{converges} to a point $s$ provided that, for every $\epsilon \geq 0$, there exists an $N$ such that if
\[n > N \textrm{ then } d(x_n, s) < \epsilon.\]
This point $s$ is called the \textit{limit} of the sequence.  We denote the limit as $\lim(s_n) = s$.
\end{defn}

Essentially, a sequence converges to a point $s$ if, after some point in the sequence, all the terms are arbitrarily close to the limit.  We do not provide a proof, but note that all bounded monotonic sequences converge.  Specifically, if a sequence is nondecreasing and bounded, it will converge to the smallest number that bounds it.  Similarly, if a sequence is nonincreasing and bounded, it will converge to the largest number which bounds it.

\begin{defn}
Let $(X,d)$ be a metric space.  A sequence $(x_n)_{n=1}^\infty$ in $X$ is a \textbf{Cauchy sequence} if, for every $\epsilon > 0$, there exists an $N$ such that
\[n,m \geq N \Rightarrow d(x_n, x_m) < \epsilon.\]
\end{defn}

The definition of a Cauchy sequence is very close to that of a convergent sequence --- a Cauchy sequence, though, says that beyond some point in the sequence, all the terms are arbitrarily close to one another.  The two are closely related, and in fact imply one another.

\begin{theorem}
Convergent sequences are Cauchy.
\end{theorem}

The idea here is that, if a sequence converges, all the terms are eventually close to the same limit.  Since the terms are all close to the same limit, they are also very close to one another.  In the following proof, note the use of the triangle inequality property of metric spaces.

\begin{proof}
We show that all convergent sequences are Cauchy.\\
Let $s = \lim(S_n)$.  Let $\epsilon > 0$.  Since $\lim(s_n) = s$, there exists $N$ for which $n > N \Rightarrow d(s_n, s) < \frac{\epsilon}{2}$.  So suppose $m,n > N$.  Then \begin{align*}
d(s_n, s_m) &\leq d(s_n, s) + d(s, s_m)\\
&< \frac{\epsilon}{2} + \frac{\epsilon}{2}\\
&= \epsilon.
\end{align*}
Thus by definition $(s_n)$ is Cauchy.
\end{proof}

We can also easily show that all Cauchy sequences are bounded.  Also, since all convergent sequences are Cauchy, we can say that all convergent sequences are bounded as well.  This fact can be combined with the \textit{Bolzano-Weierstrass Theorem} which states every bounded sequence has a convergent subsequence --- we will introduce Bolzano-Weierstrass more formally when we need it later.

\begin{theorem}
Every Cauchy sequence is bounded.
\end{theorem}

\begin{proof}
Suppose $(s_n)$ is a Cauchy sequence.  Then there exists an $N \in \mathbb{N}$ for which $||s_n - s_m|| < 1$.  In particular, if $n>N$, then $||s_n - s_{N-1}|| < 1$ (since $N+1$ is obviously greater than $N$).  Therefore, if $n>N$, then
\begin{align*}
||s_n|| &= ||s_n - s_{N+1} + s_{N+1}||\\
&\leq ||s_n - s_{N+1}|| + ||s_{N+1}||\\
&< 1 + ||s_{N+1}||.
\end{align*}
Now, let $M = \textrm{max}\{||s_1||, ||s_2||, \cdots, ||s_N||, 1 + ||s_{N+1}||\}.$  Note that we know $M$ exists since the set is finite.  Observe that if $1 \leq n \leq N$ then $||s_n|| \leq M$.  If $n > N$, we have $||s_n|| < 1 + ||s_{N+1}|| \leq M$.  So by definition, $(s_n)$ is bounded, specifically bounded by $M$.
\end{proof}

\begin{theorem}
If a subsequence of a Cauchy sequence converges to $s$, then the whole sequence converges to $s$.
\end{theorem}

\begin{proof}
Let $(X,d)$ be a metric space.  Suppose $(s_n)$ is a Cauchy sequence with subsequence $(s_{n_k})$ which converges to $s$.  We show $(s_n)$ also converges to $s$.  Let $\epsilon > 0$ and pick $N > 0$ such that for all $n,m > N$, we have
\[d(s_n, s_m) < \frac{\epsilon}{2}.\]
Since $(s_{n_k})$ converges to $s$, we can pick $K > 0$ such that for all $n_k > K$, we have
\[d(s_{n_k},s) < \frac{\epsilon}{2}.\]
Now let $M = \textrm{ max}\{N,K\}$.  Then, for all $n, m, n_k > M$, we have
\begin{align*}
d(s_n,s) &\leq d(s_n, s_{n_k}) + d(s_{n_k}, s)\\
&< \frac{\epsilon}{2} + \frac{\epsilon}{2}\\
&= \epsilon.
\end{align*}
Thus $(s_n)$ converges to $s$ as desired.
\end{proof}

These concepts will be used later on in the section on Banach spaces in order to prove a property of metric spaces called \textit{completeness}.